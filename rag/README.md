# Иструкция

- установить сервер [ollama](https://ollama.com/download)

- установить необходимую модель
пример

```bash
ollama pull smollm2:135m
```

другие [модели](https://ollama.com/search)

- установить виртуальное окружение uv и активировать его инструкция к [uv](https://github.com/ComebaLL/LLM)

- для синхронизации библиотек пишем

```bash
uv sync
```

перед запуском основной программы создаем файл, с готовыми документами в моем случае doc_skill_gems.txt, дальше запускаем скрипт filecs.py для разбиения текста. После уже переходим к основной программе

проверяем работает ли сервер ollama для этого переходим по адресу, вмоем случае

```bash
http://localhost:11434
```

если все работает запускаем программу

```bash
uv run main.py
```


первый запуск может занять относительно много времени из-за всхе подгрузок, дальше будет быстро

могут возникнуть проблемы с коннектом к серверам рекомендуется использовать proxy